## Continued from earlier today...


## [16:42] DataForSEO Keyword Research Tool — Installed & Tested
- `~/clawd/scripts/keyword-research.sh` fully working with 3 features: volume, related keywords, SERP analysis
- Test: "business presentation" = 1,300/mo volume, 1,180 related keywords found
- Cost: ~$0.08-0.23 per call depending on features
- Integrated into blog-writing skill — `/blog-strategy` now runs keyword research as step 2
- Updated `commands/blog-strategy.md` to include keyword research in workflow

## [16:50] Google Search Console — Setup In Progress
- Mike added `sophie@slideheroes.com` as Full user on GSC
- `gog` CLI has no GSC module — installed `google-api-python-client` for direct API access
- Built `~/clawd/scripts/gsc-auth.py` (manual OAuth flow: URL → paste redirect URL → extract code)
- Reuses gog's OAuth client credentials from `~/.config/gogcli/credentials.json`
- Token will save to `~/.config/gsc/token.json`
- **Waiting on Mike** to complete OAuth authorization (paste redirect URL)
- Mike also discovered his GSC was on `msmith.slideheroes@gmail.com` — added `michael@slideheroes.com` as Owner

## [16:50] Note: Mike's GSC Accounts
- `msmith.slideheroes@gmail.com` — original GSC owner
- `michael@slideheroes.com` — added as Owner (now primary)
- `sophie@slideheroes.com` — added as Full permission user

## [17:35] Google Search Console — Connected
- Sophie added as Full user on GSC by Mike
- `gog` CLI has no GSC module — built custom OAuth flow + query script
- OAuth reuses gog's client credentials, token at `~/.config/gsc/token.json`
- Had to enable Search Console API on GCP project 910791495145
- Query script: `~/clawd/scripts/gsc-query.py` (queries, pages, date range, filters)
- SlideHeroes GSC data: ~80K impressions/mo, top pages are public-speaking-anxiety (35K impr), McKinsey (17.5K), typology charts (7.9K)
- Mike's GSC was on `msmith.slideheroes@gmail.com` — added `michael@slideheroes.com` as Owner
- #447 marked done

## [17:55] Fallback Model Changed: GLM → GPT 5.2
- Mike requested fallback change from `zai/glm-4.7` to `openai-codex/gpt-5.2`
- Config patched, gateway restarted
- **Lesson:** Model failover mid-tool-call doesn't work cleanly — GPT can't pick up Anthropic-format tool call IDs (`toolu_*`). Resulted in repeated "No tool call found" errors. OpenClaw needs to retry the full turn, not continue mid-call.
- Already on latest OpenClaw (2026.2.9) so not a version issue

## [18:00] Blog Pipeline Tasks #443-446 — All Complete
- #443: `/blog-research` command — 3-layer deep research (landscape → deep sources → original angle)
- #444: `/blog-brief` command — auto-generate content brief from research
- #445: `/blog-outline` command — structural outline with SEO + AEO optimization
- #446: `/blog-qa` command — 16 automated checks (content, SEO, technical, brand)
- Full pipeline now: ideate → strategy → research → brief → outline → write → qa → publish
- All output directories created under `.ai/content/blog/`

## [19:10] Word Count Requirement
- Mike: minimum 2000 words, target range 2000-3000, auto-fail under 2000
- Updated in 4 places: blog-guidelines.md, blog-brief.md, blog-qa.md, SOP

## [19:22] Content Pillars Rewritten — 5 Pillars from Mike
1. How-To Guides (2-3/mo) — core SEO
2. Presentation Surgery (1-2/mo) — signature content, uses deck sourcing pipeline
3. AI & Presentations (1-2/mo) — product positioning
4. Founder Journey (1/mo) — authentic, personal
5. ICP Intelligence (1/mo) — research-driven
- Added "Authenticity" as 6th brand voice attribute
- Community/social research added to ideation step (Reddit, LinkedIn, competitor blogs)
- Semantic Scholar API set up (`scripts/semantic-scholar.sh`) — free academic paper search

## [19:36] Major Decision: 20 Blog Posts/Month Target
- Mike wants 1 post per weekday = 20/month
- Current 5 pillars can sustain ~12-14 — need 2-3 more
- #449 expanded: research ICP reading habits + identify additional pillar candidates
- #450 created: brainstorm session to finalize 6-7 pillars (blocked by #449)

## [19:42] Pillar Research Complete (#449)
- GLM sub-agent researched Reddit, LinkedIn, competitor blogs
- 3 new pillar candidates: Workflow & Productivity (3/mo), Sales Pitch & Client (3/mo), Delivery & Performance (2/mo)
- Proposed 8-pillar mix for 20 posts/month
- Report: `deliverables/content-pillar-research.md`

## [19:50] Tasks Created
- #448 (high) — App positioning discussion (course→app pivot context update)
- #449 (high) — ICP research + pillar discovery (done)
- #450 (high, blocked by #449) — Brainstorm session to finalize pillars

## [19:53] Data Ecosystem — BigQuery Setup Started
- Mike walking through GCP project creation
- Project name: `slideheroes-data-platform`
- Plan: create project → enable BigQuery API → create `core` dataset → service account for Sophie
- In progress — Mike doing steps in browser

## [20:07] BigQuery Setup Complete (#420)
- GCP project: `slideheroes-data-platform`
- Service account: `sophie-data-pipeline@slideheroes-data-platform.iam.gserviceaccount.com`
- Credentials: `~/.clawdbot/gcp-service-account.json`
- Datasets created: `core` (production) and `staging` (raw ingestion), both in us-east1
- Initially needed BigQuery Admin role (Data Editor + Job User wasn't enough for dataset creation)

## [20:20] Supabase → BigQuery Sync Complete (#426)
- 30 tables, 122 total rows synced to `staging` dataset
- Script: `~/clawd/scripts/sync-supabase-bigquery.py` (full replace strategy)
- Bug fixed: Decimal type serialization (added float conversion)
- First run killed by OOM/timeout after 21 tables — finished remaining 9 in second run
- Key tables: accounts (26), ai_request_logs (32), accounts_memberships (25), testimonials (8)
- Supabase connection: uses pooler URL (`aws-0-us-east-2.pooler.supabase.com:6543`) — direct DB URL resolves to IPv6 which EC2 can't reach

## [20:23] Supabase Sync Nightly Cron
- Cron ID: `72cf0587-9ce1-4b1d-bfa2-70922ae599a5`
- Schedule: 6 AM ET daily (before morning briefing)
- Silent (no notification unless errors)

## [20:25] Mike Logging Off — Working Backlog Overnight
